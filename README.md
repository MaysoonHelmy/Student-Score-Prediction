# Student Performance Prediction: Feature Selection & Model Development

This project examines a comprehensive machine learning workflow for predicting student academic performance using real-world educational data. The pipeline includes exploratory data analysis (EDA), preprocessing, feature engineering, feature selection, and model evaluation.

The goal is to build a robust regression model that predicts student outcomes based on behavioral, academic, and demographic factors.

---

## Project Structure

```
.
â”œâ”€â”€ Preprocessing and Feature Engineering/
â”‚   â”œâ”€â”€ Data_PreProcessing_and_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ df_test.csv
â”‚   â”œâ”€â”€ df_train.csv
â”‚   â””â”€â”€ StudentPerformanceFactors.csv
â”œâ”€â”€ Feature Selection and Model development/
â”‚   â””â”€â”€ Models.ipynb
â””â”€â”€ StudentPerformanceFactorsEDA.html
```

---

##  Data Overview

### Source Dataset
- **File**: `StudentPerformanceFactors.csv`
- **Source**: Kaggle.com
- Target variable: Final exam score 

### Processed Datasets
After cleaning and engineering:
- `df_train.csv`: Training dataset used for model training
- `df_test.csv`: Holdout test set for final evaluation

> These files are generated by `Data_PreProcessing_and_Feature_Engineering.ipynb`.

---

## Workflow Overview

### Step 1: Exploratory Data Analysis (EDA)
- **File**: `StudentPerformanceFactorsEDA.html`
- Interactive EDA report 

### Step 2: Preprocessing & Feature Engineering
- **File**: `Data_PreProcessing_and_Feature_Engineering.ipynb`
- Performs:
  - Handling missing values (imputation)
  - Removing outliers
  - Encoding categorical variables
  - Scaling numerical features
  - Creating new engineered features (e.g., study efficiency ratio = study hours / total available time)
- Outputs cleaned datasets: `df_train.csv` and `df_test.csv`

### Step 3: Feature Selection & Model Development
- **File**: `Models.ipynb`
- Compares multiple regression models across different feature selection strategies:
  - All Features
  - SelectKBest
  - Recursive Feature Elimination (RFE)
  - Manual Selection
- Evaluates both **Linear** and **Polynomial Regression**
- Uses **Train RÂ²**, **Test RÂ²**, and **MSE** for comparison

---

## Final Model Performance Results

| Model | Train RÂ² | Test RÂ² | Test MSE |
|-------|----------|---------|----------|
| Linear (All Features) | 0.663 | **0.736** | **3.728** |
| Linear (SelectKBest) | 0.663 | 0.711 | 4.082 |
| Linear (RFE) | 0.663 | 0.725 | 3.892 |
| Linear (Manual) | 0.649 | 0.704 | 4.185 |
| Polynomial (All Features) | 0.719 | 0.665 | 4.736 |
| Polynomial (SelectKBest) | 0.673 | 0.702 | 4.219 |
| Polynomial (RFE) | 0.685 | 0.711 | 4.091 |
| Polynomial (Manual) | 0.661 | 0.694 | 4.326 |

> **Best Performing Model**:  
> **Linear Regression with All Features**  
> â†’ **Test RÂ² = 0.736**, **MSE = 3.728**

---

## Visualization: Model Performance Comparison

### ðŸ“ˆ 1. Test & Train RÂ² by Method

<img width="841" height="544" alt="image" src="https://github.com/user-attachments/assets/a969693a-fcc6-4bfc-a6c4-489a6748d3cd" />

*Comparison of train and test RÂ² scores across feature selection methods. Linear models consistently outperform polynomial ones in generalization.*

---

### 2. Test MSE by Method

<img width="830" height="543" alt="image" src="https://github.com/user-attachments/assets/38256346-e9ce-4e57-9d75-ed732768ccdc" />

*MSE comparison shows that Linear Regression with All Features achieves the lowest prediction error on unseen data.*

---

## How to Run This Project

1. **Install Dependencies**
   ```bash
   pip install pandas numpy scikit-learn matplotlib seaborn jupyter
   ```

2. **Open Jupyter Notebook**
   ```bash
   jupyter notebook
   ```

3. **Run Notebooks in Order**
   - First: `Data_PreProcessing_and_Feature_Engineering.ipynb`
     - Loads `StudentPerformanceFactors.csv`
     - Cleans and engineers features
     - Saves `df_train.csv` and `df_test.csv`
   - Then: `Models.ipynb`
     - Loads processed data
     - Trains and evaluates models
     - Generates performance tables and plots

4. **Review Output**
   - Check the model comparison table
   - View visualizations
   - Access EDA report via `StudentPerformanceFactorsEDA.html`

---

## Key Insights

- **Feature Engineering Boosts Accuracy**: Derived ratios like "study-to-free-time ratio" improved model interpretability and performance.
- **Linear > Polynomial**: Despite higher training scores, polynomial models overfit and perform worse on test data.
- **Full Feature Set Works Best**: For this dataset, retaining all engineered features led to optimal generalization.
- **RFE and SelectKBest Are Useful**: Help reduce dimensionality without significant loss in accuracy.

---

## Requirements

- Python 3.8+
- Jupyter Notebook
- Libraries:
  ```bash
  pip install pandas numpy scikit-learn matplotlib seaborn jupyter
  ```

---

## Conclusion

This project demonstrates a full lifecycle of predictive modeling in education. From raw data to model deployment, weâ€™ve built a reliable system that predicts student performance with strong generalization.

The best-performing model â€” **Linear Regression with All Features** â€” delivers the highest test RÂ² (0.736) and lowest MSE (3.728), making it suitable for real-world applications.

Use this repository as a template for future projects involving regression, feature engineering, and model comparison!


